[
    {
        "uuid": "a1b2c3d4-e5f6-7890-1234-567890abcdef",
        "name": "Local LLM Server (stdio)",
        "description": "A local server running a language model via standard I/O.",
        "type": 0,
        "timeout": 300,
        "command": "/usr/local/bin/llama-cpp",
        "args": [
            "-m",
            "/models/llama2-7b.gguf",
            "-c",
            "2048"
        ],
        "envVars": {
            "CUDA_VISIBLE_DEVICES": "0",
            "LLAMA_NUM_THREADS": "8"
        }
    },
    {
        "uuid": "b2c3d4e5-f6a7-8901-2345-67890abcdef0",
        "name": "OpenAI Compatible API (SSE)",
        "description": "Connects to an OpenAI-compatible API endpoint for streaming responses.",
        "type": 1,
        "timeout": 60,
        "url": "https://api.example.com/v1/chat/completions",
        "requestHeaders": "{\"Authorization\": \"Bearer sk-your_api_key\", \"Content-Type\": \"application/json\"}"
    },
    {
        "uuid": "c3d4e5f6-a7b8-9012-3456-7890abcdef01",
        "name": "Custom Streamable HTTP Endpoint",
        "description": "A proprietary streaming HTTP endpoint for custom integrations.",
        "type": 2,
        "timeout": 120,
        "url": "http://localhost:8080/stream",
        "requestHeaders": "{\"X-Custom-Auth\": \"some_token\"}"
    },
    {
        "uuid": "d4e5f6a7-b8c9-0123-4567-890abcdef012",
        "name": "Remote LLM Server (stdio)",
        "description": "Remote server running LLM via stdio.",
        "type": 0,
        "timeout": 200,
        "command": "/opt/llm/llama-cpp",
        "args": [
            "-m",
            "/models/llama2-13b.gguf",
            "-c",
            "4096"
        ],
        "envVars": {
            "CUDA_VISIBLE_DEVICES": "1",
            "LLAMA_NUM_THREADS": "16"
        }
    },
    {
        "uuid": "e5f6a7b8-c9d0-1234-5678-90abcdef0123",
        "name": "Azure OpenAI API",
        "description": "Connects to Azure OpenAI endpoint.",
        "type": 1,
        "timeout": 90,
        "url": "https://azure.openai.com/v1/chat/completions",
        "requestHeaders": "{\"api-key\": \"your_azure_key\", \"Content-Type\": \"application/json\"}"
    },
    {
        "uuid": "f6a7b8c9-d0e1-2345-6789-0abcdef01234",
        "name": "Google PaLM API",
        "description": "Connects to Google PaLM API endpoint.",
        "type": 1,
        "timeout": 80,
        "url": "https://palm.googleapis.com/v1beta2/models/chat-bison:generateMessage",
        "requestHeaders": "{\"Authorization\": \"Bearer your_google_token\", \"Content-Type\": \"application/json\"}"
    },
    {
        "uuid": "a7b8c9d0-e1f2-3456-7890-abcdef012345",
        "name": "Custom SSE HTTP Endpoint",
        "description": "Custom SSE endpoint for streaming.",
        "type": 2,
        "timeout": 100,
        "url": "http://custom-sse.local/stream",
        "requestHeaders": "{\"X-API-Key\": \"custom_sse_key\"}"
    },
    {
        "uuid": "b8c9d0e1-f2a3-4567-8901-bcdef0123456",
        "name": "Anthropic Claude API",
        "description": "Connects to Anthropic Claude API.",
        "type": 1,
        "timeout": 70,
        "url": "https://api.anthropic.com/v1/complete",
        "requestHeaders": "{\"x-api-key\": \"anthropic_key\", \"Content-Type\": \"application/json\"}"
    },
    {
        "uuid": "c9d0e1f2-a3b4-5678-9012-cdef01234567",
        "name": "Local GGML Server",
        "description": "Local server running GGML model.",
        "type": 0,
        "timeout": 250,
        "command": "/usr/bin/ggml-server",
        "args": [
            "-m",
            "/models/ggml-model.bin",
            "-c",
            "1024"
        ],
        "envVars": {
            "GGML_THREADS": "4"
        }
    },
    {
        "uuid": "d0e1f2a3-b4c5-6789-0123-def012345678",
        "name": "HuggingFace Inference API",
        "description": "Connects to HuggingFace hosted inference endpoint.",
        "type": 1,
        "timeout": 60,
        "url": "https://api-inference.huggingface.co/models/gpt2",
        "requestHeaders": "{\"Authorization\": \"Bearer hf_your_token\"}"
    },
    {
        "uuid": "e1f2a3b4-c5d6-7890-1234-ef0123456789",
        "name": "Custom HTTP JSON Endpoint",
        "description": "Custom HTTP endpoint for JSON responses.",
        "type": 2,
        "timeout": 110,
        "url": "http://json-endpoint.local/api",
        "requestHeaders": "{\"X-Auth\": \"json_token\"}"
    },
    {
        "uuid": "f2a3b4c5-d6e7-8901-2345-f0123456789a",
        "name": "Remote LLM (stdio, GPU2)",
        "description": "Remote LLM server using GPU2.",
        "type": 0,
        "timeout": 220,
        "command": "/opt/llm/llama-cpp",
        "args": [
            "-m",
            "/models/llama2-30b.gguf",
            "-c",
            "8192"
        ],
        "envVars": {
            "CUDA_VISIBLE_DEVICES": "2",
            "LLAMA_NUM_THREADS": "32"
        }
    },
    {
        "uuid": "a3b4c5d6-e7f8-9012-3456-0123456789ab",
        "name": "OpenRouter API",
        "description": "Connects to OpenRouter API endpoint.",
        "type": 1,
        "timeout": 75,
        "url": "https://openrouter.ai/api/v1/chat/completions",
        "requestHeaders": "{\"Authorization\": \"Bearer openrouter_key\", \"Content-Type\": \"application/json\"}"
    },
    {
        "uuid": "b4c5d6e7-f809-0123-4567-123456789abc",
        "name": "Custom SSE Proxy",
        "description": "Proxy server for SSE streaming.",
        "type": 2,
        "timeout": 130,
        "url": "http://sse-proxy.local/stream",
        "requestHeaders": "{\"X-Proxy-Token\": \"proxy_token\"}"
    },
    {
        "uuid": "c5d6e7f8-091a-1234-5678-23456789abcd",
        "name": "Local Alpaca Server",
        "description": "Local server running Alpaca model.",
        "type": 0,
        "timeout": 180,
        "command": "/usr/local/bin/alpaca-serve",
        "args": [
            "-m",
            "/models/alpaca-7b.bin",
            "-c",
            "2048"
        ],
        "envVars": {
            "ALPACA_THREADS": "8"
        }
    },
    {
        "uuid": "d6e7f809-1a2b-2345-6789-3456789abcde",
        "name": "Cohere API",
        "description": "Connects to Cohere API endpoint.",
        "type": 1,
        "timeout": 65,
        "url": "https://api.cohere.ai/v1/generate",
        "requestHeaders": "{\"Authorization\": \"Bearer cohere_key\", \"Content-Type\": \"application/json\"}"
    },
    {
        "uuid": "e7f8091a-2b3c-3456-7890-456789abcdef",
        "name": "Custom HTTP SSE",
        "description": "Custom HTTP SSE endpoint.",
        "type": 2,
        "timeout": 140,
        "url": "http://custom-http-sse.local/stream",
        "requestHeaders": "{\"X-Stream-Token\": \"stream_token\"}"
    },
    {
        "uuid": "f8091a2b-3c4d-4567-8901-56789abcdef0",
        "name": "Remote LLM (stdio, CPU)",
        "description": "Remote LLM server using CPU only.",
        "type": 0,
        "timeout": 160,
        "command": "/opt/llm/llama-cpp",
        "args": [
            "-m",
            "/models/llama2-3b.gguf",
            "-c",
            "1024"
        ],
        "envVars": {
            "LLAMA_NUM_THREADS": "4"
        }
    },
    {
        "uuid": "091a2b3c-4d5e-5678-9012-6789abcdef01",
        "name": "AI21 Studio API",
        "description": "Connects to AI21 Studio API endpoint.",
        "type": 1,
        "timeout": 85,
        "url": "https://api.ai21.com/studio/v1/j1-large/complete",
        "requestHeaders": "{\"Authorization\": \"Bearer ai21_key\", \"Content-Type\": \"application/json\"}"
    },
    {
        "uuid": "1a2b3c4d-5e6f-6789-0123-789abcdef012",
        "name": "Custom WebSocket SSE",
        "description": "Custom WebSocket-based SSE endpoint.",
        "type": 2,
        "timeout": 150,
        "url": "ws://custom-ws-sse.local/stream",
        "requestHeaders": "{\"X-WS-Token\": \"ws_token\"}"
    },
    {
        "uuid": "2b3c4d5e-6f70-7890-1234-89abcdef0123",
        "name": "Local Falcon Server",
        "description": "Local server running Falcon model.",
        "type": 0,
        "timeout": 210,
        "command": "/usr/local/bin/falcon-serve",
        "args": [
            "-m",
            "/models/falcon-40b.bin",
            "-c",
            "4096"
        ],
        "envVars": {
            "FALCON_THREADS": "16"
        }
    }
]